{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Principles\n",
    "Testing is the process by which you exercise your code to determine if it performs as expected. The code you are testing is referred to as the **code under test**. \n",
    "\n",
    "There are two parts to writing tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Invoking** the code under test so that it is exercised in a particular way;\n",
    "1. **Evaluating** the results of executing code under test to determine if it behaved as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection of tests performed are referred to as the **test cases**. The fraction of the code under test that is executed as a result of running the test cases is referred to as **test coverage**.\n",
    "\n",
    "For dynamical languages such as Python, it's extremely important to have a high test coverage. In fact, you should try to get 100% coverage.\n",
    "\n",
    "Why is test coverage is important in a dynamic language like Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because little checking is done when the source code is read by the Python interpreter. For example, the code under test might contain a line that has a function that is undefined. This would not be detected until that line of code is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cases can be of several types. Some common classifications of test cases are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Smoke test*. This is an invocation of the code under test to see if there is an unexpected exception. It's useful as a starting point, but this doesn't tell you anything about the correctness of the results of a computation.\n",
    "- *One-shot test*. In this case, you call the code under test with arguments for which you know the expected result.\n",
    "- *Edge test*. The code under test is invoked with arguments that should cause an exception, and you evaluate if the expected exception occurrs.\n",
    "- *Pattern test* - Based on your knowledge of the *calculation* (not implementation) of the code under test, you construct a suite of test cases for which the results are known or there are known patterns in these results that are used to evaluate the results returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another principle of testing is to limit what is done in a single test case. Generally, a test case should focus on **one use of one function**. Sometimes, this is a challenge since the function being tested may call other functions that you are testing. This means that bugs in the called functions may cause failures in the tests of the calling functions. Often, you sort this out by knowing the structure of the code and focusing first on failures in lower level tests. In other situations, you may use more advanced techniques called *mocking*. A discussion of mocking is beyond the scope of this lecture.\n",
    "\n",
    "## Test-driven development\n",
    "\n",
    "A best practice is to develop your tests while you are developing your code. Indeed, one school of thought in software engineering, called **test-driven development**, advocates that you write the tests *before* you implement the code under test so that the test cases become a kind of specification for what the code under test should do.\n",
    "\n",
    "**This is how you should approach development going forward in this course.** Write your tests first.  They all fail.  Write the code for the functions to make the tests pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Test Cases\n",
    "This section presents examples of test cases. The code under test is the calculation of entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of a set of probabilities\n",
    "$$\n",
    "H = -\\sum_i p_i \\log(p_i)\n",
    "$$\n",
    "where $\\sum_i p_i = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "def entropy(p):\n",
    "    if any([not isinstance(p_i, numbers.Number) for p_i in p]):\n",
    "        raise ValueError(\"At least one input is not a number\")\n",
    "    if any([(p_i < 0.0) or (p_i > 1.0) for p_i in p]):\n",
    "        raise ValueError(\"At least one input is out of range [0...1]\")\n",
    "    else:\n",
    "        pass\n",
    "    if not np.isclose(1, np.sum(p), atol=1e-08):\n",
    "        raise ValueError(\"The list of input probabilities does not sum to 1\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    items = []\n",
    "    for p_i in p:\n",
    "        if p_i > 0:\n",
    "            interm = p_i * np.log2(p_i)\n",
    "            items.append(interm)\n",
    "    return np.abs(-np.sum(items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoke test \n",
    "\n",
    "Does the function run when we call it or does it explode in flames and release the magic smoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Smoke test\n",
    "entropy([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One shot test\n",
    "\n",
    "We know from previous discussions that when we have 4 states and they are all equally likely, the number of bits required should be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([.25, .25, .25, .25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example is that the entropy of a random variable when there is only one possible outcome is 0, therefore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that all of the probability of a distribution is at one point. An example of this is a coin with two heads. Whenever you flip it, you always get heads. That is, the probability of a head is 1.\n",
    "\n",
    "What is the entropy of such a distribution? From the calculation above, we see that the entropy should be $log(1)$, which is 0. This means that we have a test case where we know the result!\n",
    "\n",
    "Let's write a test for the 1-probability case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test completed!\n"
     ]
    }
   ],
   "source": [
    "probabilities = [1]\n",
    "expected = 0\n",
    "if not np.isclose(entropy(probabilities), expected):\n",
    "    print(\"Test failed!\")\n",
    "print(\"Test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make it so that the test is more *generic*? As in, it is easy to add a new one-shot test case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test completed!\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    [0, [1]],\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    expected = case[0]\n",
    "    probabilities = case[1]\n",
    "    if not np.isclose(entropy(probabilities), expected):\n",
    "        print(\"Test failed!\")\n",
    "print(\"Test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEAT!** We can use this structure to do a bunch of these types all at once, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 passed\n",
      "Test 2 passed\n",
      "Test completed!\n"
     ]
    }
   ],
   "source": [
    "entries = [\n",
    "    [0, [1]],\n",
    "    [2, [.25, .25, .25, .25]]\n",
    "]\n",
    "\n",
    "for idx, entry in enumerate(entries):\n",
    "    ans = entry[0]\n",
    "    prob = entry[1]\n",
    "    if not np.isclose(entropy(prob), ans):\n",
    "        print(f\"Test {idx+1} failed\")\n",
    "    else:\n",
    "        print(f\"Test {idx+1} passed\")\n",
    "print(\"Test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is an example of another one-shot test? (Hint: You need to know the expected result.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = 3\n",
    "probabilities = [.125, .125, .125, .125, .125, .125, .125, .125]\n",
    "entropy(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge tests\n",
    "\n",
    "One edge test of interest is to provide an input that is *not* a distribution in that probabilities don't sum to 1.  These should generate an exception of type ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The list of input probabilities does not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mentropy\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(\u001b[38;5;241m1\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(p), atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-08\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe list of input probabilities does not sum to 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The list of input probabilities does not sum to 1"
     ]
    }
   ],
   "source": [
    "entropy([.9, .9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another edge test is when we pass a probability that is out of range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "At least one input is out of range [0...1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mentropy\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one input is not a number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(p_i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (p_i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p_i \u001b[38;5;129;01min\u001b[39;00m p]):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one input is out of range [0...1]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: At least one input is out of range [0...1]"
     ]
    }
   ],
   "source": [
    "entropy([-0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try writing a function to test this edge case.\n",
    "\n",
    "#### Important note for edge tests that raise exceptions!\n",
    "\n",
    "You often have to write your tests using `try` and `except` blocks being sure to catch the correct Exception type, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_entropy_sum_to_1():\n",
    "    try:\n",
    "        entropy([.9, .9])\n",
    "    except (ValueError) as err:\n",
    "        print(\"Test of probability inputs not summing to 1 passed\")\n",
    "def test_entropy_negative_probability():\n",
    "    try:\n",
    "        entropy([-0.5])\n",
    "    except (ValueError) as err:\n",
    "        print(\"Test of probability input ranges passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of probability inputs not summing to 1 passed\n",
      "Test of probability input ranges passed\n"
     ]
    }
   ],
   "source": [
    "test_entropy_sum_to_1()\n",
    "test_entropy_negative_probability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider a pattern test. Examining the structure of the calculation of $H$, we consider a situation in which there are $n$ equal probabilities. That is, $p_i = \\frac{1}{n}$.\n",
    "$$\n",
    "H = -\\sum_{i=1}^{n} p_i \\log(p_i) \n",
    "= -\\sum_{i=1}^{n} \\frac{1}{n} \\log(\\frac{1}{n}) \n",
    "= n (-\\frac{1}{n} \\log(\\frac{1}{n}) )\n",
    "= -\\log(\\frac{1}{n})\n",
    "$$\n",
    "For example, entropy([0.5, 0.5]) should be $-log(0.5)$.\n",
    "\n",
    "Let's write a pattern test for this pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed for n = 100\n"
     ]
    }
   ],
   "source": [
    "# Pattern test\n",
    "def test_equal_probabilities(n):\n",
    "    prob = 1.0/n\n",
    "    ps = np.repeat(prob , n)\n",
    "    if np.isclose(entropy(ps), -np.log2(prob)):\n",
    "        print(f\"Test passed for n = {n}\")\n",
    "    else:\n",
    "        import pdb; pdb.set_trace()\n",
    "        print (f\"Test failed for n = {n}\")\n",
    "        \n",
    "# Run a test\n",
    "test_equal_probabilities(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that there are many, many cases to test. So far, we've been writing special codes for each test case. We can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Data Producing Codes\n",
    "Much of your python (or R) codes will be creating and/or transforming dataframes. A dataframe is structured like a table with:\n",
    "\n",
    "- Columns that have values of the same type\n",
    "- Rows that have a value for each column\n",
    "- An index that uniquely identifies a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeProbabilityMatrix(column_names, nrows):\n",
    "    \"\"\"\n",
    "    Makes a dataframe with the specified column names such that each\n",
    "    cell is a value in [0, 1] and columns sum to 1.\n",
    "    :param list-str column_names: names of the columns\n",
    "    :param int nrows: number of rows\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.random.uniform(0, 1, (nrows, len(column_names))))\n",
    "    df.columns = column_names\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column]/df[column].sum()\n",
    "    return df\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.576780</td>\n",
       "      <td>0.435166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.335165</td>\n",
       "      <td>0.310621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.088055</td>\n",
       "      <td>0.254213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b\n",
       "0  0.576780  0.435166\n",
       "1  0.335165  0.310621\n",
       "2  0.088055  0.254213"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makeProbabilityMatrix(['a', 'b'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 2: Check columns\n",
    "COLUMNS = ['a', 'b']\n",
    "df = makeProbabilityMatrix(COLUMNS, 3)\n",
    "set(COLUMNS) == set(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Write a function that tests the following:\n",
    "- The returned dataframe has the expected columns\n",
    "- The returned dataframe has the expected rows\n",
    "- Values in columns are of the correct type and range\n",
    "- Values in column sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed: columns\n",
      "Test passed: rows\n",
      "Test passed: values\n",
      "Test passed: sums\n"
     ]
    }
   ],
   "source": [
    "import numbers\n",
    "\n",
    "def test_makeProbabilityMatrix_columns():\n",
    "    df = makeProbabilityMatrix(['a', 'b'], 3)\n",
    "    # Check expected columns\n",
    "    if list(df.columns) != ['a', 'b']:\n",
    "        print(f\"Test failed: column names not as expected\")\n",
    "        return\n",
    "    print(f\"Test passed: columns\")\n",
    "\n",
    "def test_makeProbabilityMatrix_rows():\n",
    "    df = makeProbabilityMatrix(['a', 'b'], 3)\n",
    "    # Check expected number of rows\n",
    "    if len(df) != 3:\n",
    "        print(f\"Test failed: expected 3 rows but got {len(df)}\")\n",
    "        return\n",
    "    print(f\"Test passed: rows\")\n",
    "\n",
    "def test_makeProbabilityMatrix_values():\n",
    "    df = makeProbabilityMatrix(['a', 'b'], 3)\n",
    "    # Check values are correct type and range\n",
    "    for col in df.columns:\n",
    "        for val in df[col]:\n",
    "            if not isinstance(val, numbers.Number):\n",
    "                print(f\"Test failed: not all values are numbers\")\n",
    "                return\n",
    "            if val < 0 or val > 1:\n",
    "                print(f\"Test failed: not all values are in range [0, 1]\")\n",
    "                return\n",
    "    print(f\"Test passed: values\")\n",
    "\n",
    "def test_makeProbabilityMatrix_sums():\n",
    "    df = makeProbabilityMatrix(['a', 'b'], 3)\n",
    "    # Check values in column sum to 1\n",
    "    for col in df.columns:\n",
    "        if not np.isclose(1, df[col].sum()):\n",
    "            print(f\"Test failed: column {col} does not sum to 1\")\n",
    "            return\n",
    "    print(f\"Test passed: sums\")\n",
    "\n",
    "test_makeProbabilityMatrix_columns()\n",
    "test_makeProbabilityMatrix_rows()\n",
    "test_makeProbabilityMatrix_values()\n",
    "test_makeProbabilityMatrix_sums()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unittest Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several reasons to use a test infrastructure:\n",
    "- If you have many test cases (which you should!), the test infrastructure will save you from writing a lot of code.\n",
    "- The infrastructure provides a uniform way to report test results, and to handle test failures.\n",
    "- A test infrastructure can tell you about coverage so you know what tests to add.\n",
    "\n",
    "[We'll be using the `unittest` framework](https://docs.python.org/3/library/unittest.html). This is a separate Python package. Using this infrastructure, requires the following:\n",
    "1. import the unittest module\n",
    "1. define a class that inherits from unittest.TestCase\n",
    "1. write methods that run the code to be tested and check the outcomes.\n",
    "\n",
    "The last item has two subparts. First, we must identify which methods in the class inheriting from unittest.TestCase are tests. You indicate that a method is to be run as a test by having the method name begin with \"test\".\n",
    "\n",
    "Second, the \"test methods\" should communicate with the infrastructure the results of evaluating output from the code under test. This is done by using `assert` statements. For example, `self.assertEqual` takes two arguments. If these are objects for which `==` returns `True`, then the test passes. Otherwise, the test fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Define a class in which the tests will run\n",
    "class UnitTests(unittest.TestCase):\n",
    "\n",
    "    # Each method in the class to execute a test\n",
    "    def test_success(self):\n",
    "        self.assertEqual(1, 1)\n",
    "        \n",
    "    def test_success1(self):\n",
    "        self.assertTrue(1 == 1)\n",
    "\n",
    "    def test_failure(self):\n",
    "        self.assertLess(1, 2)\n",
    " \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(UnitTests)\n",
    "_ = unittest.TextTestRunner().run(suite)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for homework or your work should use test files.** In this lesson, we'll show how to write test codes in a Jupyter notebook. This is done for pedidogical reasons. It is **NOT** not something you should do in practice, except as an intermediate exploratory approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: unittest\n",
    "- Rewrite the above one-shot tests for entropy of equal probabilities using the unittest infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Implementating a pattern test. Use functions in the test.\n",
    "import unittest\n",
    "import numpy as np\n",
    "\n",
    "# Define a class in which the tests will run\n",
    "class TestEntropy(unittest.TestCase):\n",
    "        \n",
    "    def test_equal_probability(self):\n",
    "        def test(count):\n",
    "            \"\"\"\n",
    "            Invokes the entropy function for a number of values equal to count\n",
    "            that have the same probability.\n",
    "            :param int count:\n",
    "            \"\"\"\n",
    "            prob = 1.0/count\n",
    "            ps = np.repeat(prob , count)\n",
    "            self.assertAlmostEqual(entropy(ps), -np.log2(prob), delta=1e-08, msg=\"Probabilities do not egaul the negative log of the base probability\")\n",
    "\n",
    "        test(2)\n",
    "        test(20)\n",
    "        test(200)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestEntropy)\n",
    "_ = unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing For Exceptions\n",
    "\n",
    "Edge test cases often involves handling exceptions. One approach is to code this directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Define a class in which the tests will run\n",
    "class TestEntropy(unittest.TestCase):\n",
    "        \n",
    "    def test_invalid_probability(self):\n",
    "        try:\n",
    "            entropy([0.1, 0.5])\n",
    "            self.assertTrue(False)\n",
    "        except ValueError:\n",
    "            self.assertTrue(True)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestEntropy)\n",
    "_ = unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`unittest` provides help with testing exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Define a class in which the tests will run\n",
    "class TestEntropy(unittest.TestCase):\n",
    "        \n",
    "    def test_invalid_probability(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            entropy([0.1, 0.5])\n",
    "        \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestEntropy)\n",
    "_ = unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: A Full Test Suite\n",
    "\n",
    "* Write a full test suite for the entropy function, including smoke test and edge tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.015s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Define a class in which the tests will run\n",
    "class TestEntropy(unittest.TestCase):\n",
    "\n",
    "    # Smoke test\n",
    "    def test_entropy_smoke(self):\n",
    "        entropy([0.5, 0.2, 0.3])\n",
    "        self.assertTrue(True)\n",
    "    \n",
    "    # One-shot test\n",
    "    def test_certainty(self):\n",
    "        self.assertAlmostEqual(0, entropy([1]))\n",
    "\n",
    "    # Pattern test\n",
    "    def test_equal_probability(self):\n",
    "        def test(count):\n",
    "            \"\"\"\n",
    "            Invokes the entropy function for a number of values equal to count\n",
    "            that have the same probability.\n",
    "            :param int count:\n",
    "            \"\"\"\n",
    "            prob = 1.0/count\n",
    "            ps = np.repeat(prob , count)\n",
    "            self.assertAlmostEqual(entropy(ps), -np.log2(prob), delta=1e-08, msg=\"Probabilities do not egaul the negative log of the base probability\")\n",
    "\n",
    "        for i in range(1, 100):\n",
    "            test(i)\n",
    "    \n",
    "    # Edge tests\n",
    "    def test_invalid_probability_sum(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            entropy([0.1, 0.5])\n",
    "\n",
    "    def test_invalid_probability_individual(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            entropy([0.6, 1.2, -0.6])\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestEntropy)\n",
    "_ = unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "**Question**: What tests would you write for a plotting function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You could compare images (not great)\n",
    "* The plot returned by `matplotlib` actually contains numeric data - you could do tests on that ([link](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib-pyplot-plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How would you test functions with side effects, like file operations or printing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can examine the contents of files after running a function\n",
    "* You can redirect standard output (stdout) using `patch` ([link](https://docs.python.org/3/library/unittest.mock.html#patch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Files\n",
    "\n",
    "Although I presented the elements of `unittest` in a notebook. **your tests should be in a file**. If the name of module with the code under test is `foo.py`, then the name of the test file should be `test_foo.py`.\n",
    "\n",
    "The structure of the test file will be very similar to cells above. You will import `unittest`. You must also import the module with the code under test. We usually put the tests in a folder called `tests`.\n",
    "\n",
    "In order for the tests to run when you run the test file on the command line, you should include a main block (like we learned last week) that uses the `unittest` module's main to run all tests.\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "```\n",
    "\n",
    "You will NOT include the `TestLoader`/`TestRunner` lines that we've been using in Jupyter Notebooks.\n",
    "\n",
    "Lastly, you should include an empty file called `__init__.py` in the `tests` directory. We will discuss in a later lecture what this file is (it has to do with packages!).\n",
    "\n",
    "You can then run the tests one of two ways:\n",
    "\n",
    "1. You can run the test directly with Python:\n",
    "  ```bash\n",
    "  $ python -m tests.test_module_name\n",
    "  ```\n",
    "1. You can use \"auto discovery\" and let the framework automatically find all of the tests.\n",
    "  ```bash\n",
    "  $ python -m unittest discover\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: A Test Suite In A File\n",
    "\n",
    "1. Move the entropy function into a file called `entropy.py` in a folder called `entropy` and the test suite for it into a file called `test_entropy.py`, which is located in the `tests` directory. Make sure you can run the test suite.\n",
    "\n",
    "```\n",
    "my_project\n",
    "    my_module\n",
    "        my_module.py\n",
    "    tests\n",
    "        test_my_module.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Test Driven Development\n",
    "\n",
    "Start by writing the tests. Then write the code.\n",
    "\n",
    "We illustrate this by considering a function geomean that takes a list of numbers as input and produces the geometric mean on output.\n",
    "\n",
    "The geometric mean is defined as the nth root of the product of n numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.019s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Define a class in which the tests will run\n",
    "class TestGeomean(unittest.TestCase):\n",
    "    # Smoke test\n",
    "    def test_geomean_smoke(self):\n",
    "        geomean([3, 2, 1])\n",
    "        self.assertTrue(True)\n",
    "\n",
    "    # Edge tests\n",
    "    def test_geomean_negatives(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            geomean([9, 15, -3, 8])\n",
    "\n",
    "    # One-shot tests\n",
    "    def test_geomean_zero(self):\n",
    "        self.assertEqual(0, geomean([3, 0, 1]))\n",
    "\n",
    "    def test_geomean_square_two_elements(self):\n",
    "        self.assertEqual(5, geomean([25, 1]))\n",
    "\n",
    "    # Pattern tests\n",
    "    def test_geomean_one_element(self):\n",
    "        def test(n):\n",
    "            self.assertEqual(n, geomean([n]))\n",
    "        for i in range(1, 20):\n",
    "            test(i)\n",
    "    \n",
    "    def test_geomean_same_elements(self):\n",
    "        def test(n, m):\n",
    "            self.assertEqual(n, geomean([n] * m))\n",
    "        for i in range(1, 20):\n",
    "            for j in range(2, 20):\n",
    "                test(i, j)\n",
    "    \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestEntropy)\n",
    "_ = unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geomean(lst):\n",
    "    product = 1\n",
    "    for element in lst:\n",
    "        if element < 0:\n",
    "            raise ValueError('Cannot take the mean with negative numbers')\n",
    "        lst *= element\n",
    "    return product ** (1 / len(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other infrastructures\n",
    "- pytest\n",
    "- nose\n",
    "- Use binary functions that being with \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://www.youtube.com/watch?v=GEqM9uJi64Q (Pydata 2015)\n",
    "https://www.youtube.com/watch?v=yACtdj1_IxE (Pycon 2017)\n",
    "\n",
    "The first talk mentions some packages:\n",
    "engarde - https://github.com/TomAugspurger/engarde\n",
    "Hypothesis - https://hypothesis.readthedocs.io/en/latest/\n",
    "Feature Forge - https://github.com/machinalis/featureforge\n",
    "\n",
    "\n",
    "Detlef Nauck talk: \n",
    "http://ukkdd.org.uk/2017/info/talks/nauck.pdf\n",
    "He also had a list of R tools but I could not find the slides form the talk I saw.\n",
    "\n",
    "Test Driven Data Analysis:\n",
    "https://www.youtube.com/watch?v=TGwZnZYg0jw\n",
    "\n",
    "Profiling for Pandas:\n",
    "https://github.com/pandas-profiling/pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
